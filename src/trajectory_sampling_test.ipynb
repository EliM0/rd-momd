{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 17:46:43.068181: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)\n",
    "\n",
    "import jax\n",
    "\n",
    "from open_spiel.python import policy\n",
    "from open_spiel.python import rl_environment\n",
    "from open_spiel.python.mfg import utils\n",
    "from open_spiel.python.mfg.algorithms import distribution\n",
    "from open_spiel.python.mfg.algorithms import trajectory_munchausen_deep_mirror_descent\n",
    "from open_spiel.python.mfg.algorithms import nash_conv\n",
    "from open_spiel.python.mfg.algorithms import policy_value\n",
    "from open_spiel.python.mfg.games import factory\n",
    "from open_spiel.python.utils import metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the game to play.\n",
    "GAME_NAME = \"mfg_crowd_modelling_2d\"\n",
    "# Name of the game setting.\n",
    "ENV_SETTING = \"crowd_modelling_2d_four_rooms\"\n",
    "# Number of transitions to sample at each learning step.\n",
    "BATCH_SIZE = 128\n",
    "# Number of steps between learning updates.\n",
    "LEARN_EVERY = 64\n",
    "# Number of training episodes for each iteration.\n",
    "NUM_EPISODES_PER_ITERATION = 1000\n",
    "# Number of iterations.\n",
    "NUM_ITERATIONS = 100\n",
    "# Number of game steps over which epsilon is decayed.\n",
    "EPSILON_DECAY_DURATION = 100000\n",
    "# Power for the epsilon decay.\n",
    "EPSILON_POWER = 1.0\n",
    "# Starting exploration parameter.\n",
    "EPSILON_START = 0.1\n",
    "# Final exploration parameter.\n",
    "EPSILON_END = 0.1\n",
    "# Discount factor for future rewards.\n",
    "DISCOUNT_FACTOR = 1.0\n",
    "# Reset the replay buffer when the softmax policy is updated.\n",
    "RESET_REPLAY_BUFFER_ON_UPDATE = False\n",
    "# Training seed.\n",
    "SEED = 42\n",
    "# Episode frequency at which the agents are evaluated.\n",
    "EVAL_EVERY = 200\n",
    "# Number of hidden units in the avg-net and Q-net.\n",
    "HIDDEN_LAYERS_SIZES = [128, 128]\n",
    "#Number of steps beween DQN target network updates.\n",
    "UPDATE_TARGET_NETWORK_EVERY = 200\n",
    "# Size of the trajectory replay buffer.\n",
    "TRAJECTORY_REPLAY_BUFFER_CAPACITY = 1000\n",
    "# Number of trajectories in buffer before learning begins.\n",
    "MIN_TRAJECTORY_REPLAY_BUFFER_SIZE_TO_LEARN = 25\n",
    "# Number of transitions in trajectory.\n",
    "TRAJECTORY_SAMPLE_LENGTH = 40\n",
    "# Number of transitions to overlap between trajectories.\n",
    "TRAJECTORY_SAMPLE_OVERLAP_LENGTH = 20\n",
    "# Optimizer\n",
    "OPTIMIZER = \"adam\"\n",
    "# Learning rate for inner RL agent.\n",
    "LEARNING_RATE = 0.01\n",
    "# Loss function.\n",
    "LOSS = \"mse\"\n",
    "# Parameter for Huber loss.\n",
    "HUBER_LOSS_PARAMETER = 1.0\n",
    "# Value to clip the gradient to.\n",
    "GRADIENT_CLIPPING = None\n",
    "# Temperature parameter in Munchausen target.\n",
    "TAU = 10\n",
    "# Alpha parameter in Munchausen target.\n",
    "ALPHA = 0.99\n",
    "# Use Munchausen penalty terms.\n",
    "WITH_MUNCHAUSEN = True\n",
    "# Save/load neural network weights.\n",
    "USE_CHECKPOINTS = False\n",
    "# Directory to save/load the agent.\n",
    "CHECKPOINT_DIR = \"/tmp/dqn_test\"\n",
    "# Logging directory to use for TF summary files.\n",
    "LOGDIR = None\n",
    "# Enables logging of the distribution.\n",
    "LOG_DISTRIBUTION = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Creating mfg_crowd_modelling_2d game with parameters: {'forbidden_states': '[0|0;1|0;2|0;3|0;4|0;5|0;6|0;7|0;8|0;9|0;10|0;11|0;12|0;0|1;6|1;12|1;0|2;6|2;12|2;0|3;12|3;0|4;6|4;12|4;0|5;6|5;12|5;0|6;1|6;2|6;4|6;5|6;6|6;7|6;8|6;10|6;11|6;12|6;0|7;6|7;12|7;0|8;6|8;12|8;0|9;12|9;0|10;6|10;12|10;0|11;6|11;12|11;0|12;1|12;2|12;3|12;4|12;5|12;6|12;7|12;8|12;9|12;10|12;11|12;12|12]', 'horizon': 40, 'initial_distribution': '[1|1]', 'initial_distribution_value': '[1.0]', 'size': 13, 'only_distribution_reward': True}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mfg_crowd_modelling_2d(forbidden_states=[0|0;1|0;2|0;3|0;4|0;5|0;6|0;7|0;8|0;9|0;10|0;11|0;12|0;0|1;6|1;12|1;0|2;6|2;12|2;0|3;12|3;0|4;6|4;12|4;0|5;6|5;12|5;0|6;1|6;2|6;4|6;5|6;6|6;7|6;8|6;10|6;11|6;12|6;0|7;6|7;12|7;0|8;6|8;12|8;0|9;12|9;0|10;6|10;12|10;0|11;6|11;12|11;0|12;1|12;2|12;3|12;4|12;5|12;6|12;7|12;8|12;9|12;10|12;11|12;12|12],horizon=40,initial_distribution=[1|1],initial_distribution_value=[1.0],only_distribution_reward=True,size=13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = factory.create_game_with_setting(GAME_NAME, ENV_SETTING)\n",
    "game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_players = game.num_players()\n",
    "num_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<open_spiel.python.policy.UniformRandomPolicy at 0x7f6a65661050>,\n",
       " <open_spiel.python.mfg.algorithms.distribution.DistributionPolicy at 0x7f6a65663b10>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniform_policy = policy.UniformRandomPolicy(game)\n",
    "uniform_dist = distribution.DistributionPolicy(game, uniform_policy)\n",
    "uniform_policy, uniform_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using game instance: mfg_crowd_modelling_2d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<open_spiel.python.rl_environment.Environment at 0x7f6a65744d50>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs = [\n",
    "    rl_environment.Environment(\n",
    "        game,\n",
    "        mfg_distribution=uniform_dist,\n",
    "        mfg_population=p,\n",
    "        observation_type=rl_environment.ObservationType.OBSERVATION\n",
    "    )\n",
    "    for p in range(num_players)\n",
    "]\n",
    "envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<open_spiel.python.rl_environment.Environment at 0x7f6a65744d50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = envs[0]\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_state_size = env.observation_spec()[\"info_state\"][0]\n",
    "info_state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_actions = env.action_spec()[\"num_actions\"]\n",
    "num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<open_spiel.python.mfg.algorithms.trajectory_munchausen_deep_mirror_descent.TrajectoryMunchausenDQN at 0x7f6abfb00f90>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {\n",
    "    \"alpha\": ALPHA,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"discount_factor\": DISCOUNT_FACTOR,\n",
    "    \"epsilon_decay_duration\": EPSILON_DECAY_DURATION,\n",
    "    \"epsilon_end\": EPSILON_END,\n",
    "    \"epsilon_power\": EPSILON_POWER,\n",
    "    \"epsilon_start\": EPSILON_START,\n",
    "    \"gradient_clipping\": GRADIENT_CLIPPING,\n",
    "    \"hidden_layers_sizes\": HIDDEN_LAYERS_SIZES,\n",
    "    \"huber_loss_parameter\": HUBER_LOSS_PARAMETER,\n",
    "    \"learn_every\": LEARN_EVERY,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"loss\": LOSS,\n",
    "    \"min_trajectory_replay_buffer_size_to_learn\": MIN_TRAJECTORY_REPLAY_BUFFER_SIZE_TO_LEARN,\n",
    "    \"optimizer\": OPTIMIZER,\n",
    "    \"trajectory_replay_buffer_capacity\": TRAJECTORY_REPLAY_BUFFER_CAPACITY,\n",
    "    \"trajectory_sample_length\": TRAJECTORY_SAMPLE_LENGTH,\n",
    "    \"trajectory_sample_overlap_length\": TRAJECTORY_SAMPLE_OVERLAP_LENGTH,\n",
    "    \"reset_replay_buffer_on_update\": RESET_REPLAY_BUFFER_ON_UPDATE,\n",
    "    \"seed\": SEED,\n",
    "    \"tau\": TAU,\n",
    "    \"update_target_network_every\": UPDATE_TARGET_NETWORK_EVERY,\n",
    "    \"with_munchausen\": WITH_MUNCHAUSEN,\n",
    "}\n",
    "\n",
    "agents = [\n",
    "    trajectory_munchausen_deep_mirror_descent.TrajectoryMunchausenDQN(\n",
    "        p,\n",
    "        info_state_size,\n",
    "        num_actions,\n",
    "        **kwargs,\n",
    "    )\n",
    "    for p in range(num_players)\n",
    "]\n",
    "agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:[Hyperparameters] {'alpha': 0.99, 'batch_size': 128, 'discount_factor': 1.0, 'epsilon_decay_duration': 100000, 'epsilon_end': 0.1, 'epsilon_power': 1.0, 'epsilon_start': 0.1, 'gradient_clipping': None, 'hidden_layers_sizes': [128, 128], 'huber_loss_parameter': 1.0, 'learn_every': 64, 'learning_rate': 0.01, 'loss': 'mse', 'min_trajectory_replay_buffer_size_to_learn': 25, 'optimizer': 'adam', 'trajectory_replay_buffer_capacity': 1000, 'trajectory_sample_length': 40, 'trajectory_sample_overlap_length': 20, 'reset_replay_buffer_on_update': False, 'seed': 42, 'tau': 10, 'update_target_network_every': 200, 'with_munchausen': True}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<clu.metric_writers.async_writer.AsyncMultiWriter at 0x7f6a65fd0450>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "just_logging = LOGDIR is None or jax.host_id() > 0\n",
    "writer = metrics.create_default_writer(\n",
    "    logdir=LOGDIR, just_logging=just_logging,\n",
    ")\n",
    "writer.write_hparams(kwargs)\n",
    "writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging_fn(it, episode, vals):\n",
    "    writer.write_scalars(it * NUM_EPISODES_PER_ITERATION + episode, vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<open_spiel.python.mfg.algorithms.trajectory_munchausen_deep_mirror_descent.TrajectoryDeepOnlineMirrorDescent at 0x7f6a64fa2550>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md = trajectory_munchausen_deep_mirror_descent.TrajectoryDeepOnlineMirrorDescent(\n",
    "    game,\n",
    "    envs,\n",
    "    agents,\n",
    "    eval_every=EVAL_EVERY,\n",
    "    num_episodes_per_iteration=NUM_EPISODES_PER_ITERATION,\n",
    "    logging_fn=logging_fn,\n",
    ")\n",
    "md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(it):\n",
    "   initial_states = game.new_initial_states()\n",
    "   pi_value = policy_value.PolicyValue(game, md.distribution, md.policy)\n",
    "   m = {\n",
    "      f\"best_response/{state}\": pi_value.eval_state(state)\n",
    "      for state in initial_states\n",
    "   }\n",
    "   nash_conv_md = nash_conv.NashConv(game, md.policy).nash_conv()\n",
    "   m[\"nash_conv_md\"] = nash_conv_md\n",
    "   if LOG_DISTRIBUTION and LOGDIR:\n",
    "      filename = os.path.join(LOGDIR, f\"distribution_{it}.pkl\")\n",
    "      utils.save_parametric_distribution(md.distribution, filename)\n",
    "   logging_fn(it, 0, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:[0] best_response/initial=144.195, nash_conv_md=167.801\n",
      "INFO:absl:[200] agent0/loss=457.754638671875\n",
      "INFO:absl:[400] agent0/loss=162.80447387695312\n",
      "INFO:absl:[600] agent0/loss=95.06572723388672\n",
      "INFO:absl:[800] agent0/loss=28.330066680908203\n",
      "INFO:absl:[1000] agent0/loss=38.8974494934082\n",
      "INFO:absl:[1000] best_response/initial=155.288, nash_conv_md=99.1171\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m log_metrics(\u001b[39m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m it \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, NUM_ITERATIONS \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m    md\u001b[39m.\u001b[39;49miteration()\n\u001b[1;32m      4\u001b[0m    log_metrics(it)\n",
      "File \u001b[0;32m~/ETH/Foundations of Reinforcement Learning/final_project/venv2/lib64/python3.11/site-packages/open_spiel/python/mfg/algorithms/trajectory_munchausen_deep_mirror_descent.py:642\u001b[0m, in \u001b[0;36mTrajectoryDeepOnlineMirrorDescent.iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39miteration\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    641\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"An iteration of Mirror Descent.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_agents()\n\u001b[1;32m    643\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_policy_and_distribution()\n\u001b[1;32m    644\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iteration \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/ETH/Foundations of Reinforcement Learning/final_project/venv2/lib64/python3.11/site-packages/open_spiel/python/mfg/algorithms/trajectory_munchausen_deep_mirror_descent.py:603\u001b[0m, in \u001b[0;36mTrajectoryDeepOnlineMirrorDescent._train_agents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    601\u001b[0m time_step \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m    602\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m time_step\u001b[39m.\u001b[39mlast():\n\u001b[0;32m--> 603\u001b[0m   agent_output \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mstep(time_step, use_softmax\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    604\u001b[0m   action_list \u001b[39m=\u001b[39m [agent_output\u001b[39m.\u001b[39maction]\n\u001b[1;32m    605\u001b[0m   time_step \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action_list)\n",
      "File \u001b[0;32m~/ETH/Foundations of Reinforcement Learning/final_project/venv2/lib64/python3.11/site-packages/open_spiel/python/mfg/algorithms/trajectory_munchausen_deep_mirror_descent.py:244\u001b[0m, in \u001b[0;36mTrajectoryMunchausenDQN.step\u001b[0;34m(self, time_step, is_evaluation, add_transition_record, use_softmax, tau)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step_counter \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_learn_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 244\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_loss_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearn()\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step_counter \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_target_network_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    247\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params_target_q_network \u001b[39m=\u001b[39m _copy_params(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params_q_network)\n",
      "File \u001b[0;32m~/ETH/Foundations of Reinforcement Learning/final_project/venv2/lib64/python3.11/site-packages/open_spiel/python/mfg/algorithms/trajectory_munchausen_deep_mirror_descent.py:435\u001b[0m, in \u001b[0;36mTrajectoryMunchausenDQN.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39mfor\u001b[39;00m trajetory \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trajectory_replay_buffer:\n\u001b[1;32m    434\u001b[0m   \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(trajetory) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m--> 435\u001b[0m     transitions\u001b[39m.\u001b[39mappend(Transition(\n\u001b[1;32m    436\u001b[0m       info_state\u001b[39m=\u001b[39;49mtrajetory[i]\u001b[39m.\u001b[39;49minfo_state,\n\u001b[1;32m    437\u001b[0m       action\u001b[39m=\u001b[39;49mtrajetory[i]\u001b[39m.\u001b[39;49maction,\n\u001b[1;32m    438\u001b[0m       legal_one_hots\u001b[39m=\u001b[39;49mtrajetory[i]\u001b[39m.\u001b[39;49mlegal_one_hots,\n\u001b[1;32m    439\u001b[0m       reward\u001b[39m=\u001b[39;49mtrajetory[i]\u001b[39m.\u001b[39;49mreward,\n\u001b[1;32m    440\u001b[0m       next_info_state\u001b[39m=\u001b[39;49mtrajetory[i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49minfo_state,\n\u001b[1;32m    441\u001b[0m       is_final_step\u001b[39m=\u001b[39;49m\u001b[39mfloat\u001b[39;49m(trajetory[i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mis_final_step),\n\u001b[1;32m    442\u001b[0m       next_legal_one_hots\u001b[39m=\u001b[39;49mtrajetory[i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mlegal_one_hots,\n\u001b[1;32m    443\u001b[0m     ))\n\u001b[1;32m    444\u001b[0m transitions \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39msample(transitions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_size)\n\u001b[1;32m    446\u001b[0m \u001b[39m# transitions = self._replay_buffer.sample(self._batch_size)\u001b[39;00m\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls, info_state, action, legal_one_hots, reward, next_info_state, is_final_step, next_legal_one_hots)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log_metrics(0)\n",
    "for it in range(1, NUM_ITERATIONS + 1):\n",
    "   md.iteration()\n",
    "   log_metrics(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
