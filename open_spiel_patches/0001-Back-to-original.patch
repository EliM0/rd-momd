From d80d2911f1bdd05123026d90c78ef6f82261bcbe Mon Sep 17 00:00:00 2001
From: Elisa <elisamecp@gmail.com>
Date: Fri, 9 Jun 2023 23:29:50 +0200
Subject: [PATCH] Back to original

---
 open_spiel/python/rl_agent_policy.py | 19 ++-------------
 open_spiel/python/rl_environment.py  | 35 +++-------------------------
 2 files changed, 5 insertions(+), 49 deletions(-)

diff --git a/open_spiel/python/rl_agent_policy.py b/open_spiel/python/rl_agent_policy.py
index e7032be7..ed8f7f11 100644
--- a/open_spiel/python/rl_agent_policy.py
+++ b/open_spiel/python/rl_agent_policy.py
@@ -19,7 +19,6 @@ from open_spiel.python import policy
 from open_spiel.python import rl_agent
 from open_spiel.python import rl_environment
 
-import numpy as np
 
 class JointRLAgentPolicy(policy.Policy):
   """Joint policy denoted by the RL agents of a game.
@@ -47,8 +46,7 @@ class JointRLAgentPolicy(policy.Policy):
     self._agents = agents
     self._obs = {
         "info_state": [None] * game.num_players(),
-        "legal_actions": [None] * game.num_players(),
-        "distribution": [None] * game.num_players()
+        "legal_actions": [None] * game.num_players()
     }
     self._use_observation = use_observation
 
@@ -73,11 +71,6 @@ class JointRLAgentPolicy(policy.Policy):
         if self._use_observation else state.information_state_tensor(player_id))
     self._obs["legal_actions"][player_id] = legal_actions
 
-    if self.game.get_type().short_name == 'mfg_crowd_modelling_2d':
-        distribution = state.distribution_tensor(player_id)
-        dist = self.get_partial_distribution(self._obs["info_state"][player_id], distribution) if self._agents[player_id]._agent.partial_obs else distribution
-        self._obs["distribution"][player_id] = dist
-
     info_state = rl_environment.TimeStep(
         observations=self._obs, rewards=None, discounts=None, step_type=None)
 
@@ -85,14 +78,6 @@ class JointRLAgentPolicy(policy.Policy):
     prob_dict = {action: p[action] for action in legal_actions}
     return prob_dict
 
-  def get_partial_distribution(self, info_state, distribution):
-    size = int(np.sqrt(self.game.distribution_tensor_size()))
-
-    x = info_state[:size].index(1)
-    y = info_state[size:2*size].index(1)
-
-    return [distribution[i*size + j] if i >= 0 and i < size and j >= 0 and j < size else 0 
-            for i in range(x-1, x+2) for j in range(y-1, y+2)]
 
 class RLAgentPolicy(JointRLAgentPolicy):
   """A policy for a specific agent trained in an RL environment."""
@@ -112,4 +97,4 @@ class RLAgentPolicy(JointRLAgentPolicy):
 
   def action_probabilities(self, state, player_id=None):
     return super().action_probabilities(
-        state, self._player_id if player_id is None else player_id)
+        state, self._player_id if player_id is None else player_id)
\ No newline at end of file
diff --git a/open_spiel/python/rl_environment.py b/open_spiel/python/rl_environment.py
index 83aac9f4..9438d972 100644
--- a/open_spiel/python/rl_environment.py
+++ b/open_spiel/python/rl_environment.py
@@ -149,7 +149,6 @@ class Environment(object):
                mfg_distribution=None,
                mfg_population=None,
                enable_legality_check=False,
-               partial_obs=False,
                **kwargs):
     """Constructor.
 
@@ -173,7 +172,6 @@ class Environment(object):
     self._mfg_distribution = mfg_distribution
     self._mfg_population = mfg_population
     self._enable_legality_check = enable_legality_check
-    self.partial_obs = partial_obs
 
     if isinstance(game, str):
       if kwargs:
@@ -235,10 +233,8 @@ class Environment(object):
         "info_state": [],
         "legal_actions": [],
         "current_player": [],
-        "serialized_state": [],
-        "distribution": []
+        "serialized_state": []
     }
-
     rewards = []
     step_type = StepType.LAST if self._state.is_terminal() else StepType.MID
     self._should_reset = step_type == StepType.LAST
@@ -249,11 +245,6 @@ class Environment(object):
       observations["info_state"].append(
           self._state.observation_tensor(player_id) if self._use_observation
           else self._state.information_state_tensor(player_id))
-      
-      if self.game.get_type().short_name == 'mfg_crowd_modelling_2d':
-        distribution = self._state.distribution_tensor(player_id)
-        dist = self.get_partial_distribution(observations["info_state"][-1], distribution) if self.partial_obs else distribution
-        observations["distribution"].append(dist)
 
       observations["legal_actions"].append(self._state.legal_actions(player_id))
     observations["current_player"] = self._state.current_player()
@@ -356,20 +347,12 @@ class Environment(object):
         "info_state": [],
         "legal_actions": [],
         "current_player": [],
-        "serialized_state": [],
-        "distribution": []
+        "serialized_state": []
     }
-
     for player_id in range(self.num_players):
       observations["info_state"].append(
           self._state.observation_tensor(player_id) if self._use_observation
           else self._state.information_state_tensor(player_id))
-      
-      if self.game.get_type().short_name == 'mfg_crowd_modelling_2d':
-        distribution = self._state.distribution_tensor(player_id)
-        dist = self.get_partial_distribution(observations["info_state"][-1], distribution) if self.partial_obs else distribution
-        observations["distribution"].append(dist)
-        
       observations["legal_actions"].append(self._state.legal_actions(player_id))
     observations["current_player"] = self._state.current_player()
 
@@ -408,8 +391,6 @@ class Environment(object):
     Returns:
       A specification dict describing the observation fields and shapes.
     """
-    dist_size = 9 if self.partial_obs else self._game.distribution_tensor_size()
-    
     return dict(
         info_state=tuple([
             self._game.observation_tensor_size() if self._use_observation else
@@ -418,7 +399,6 @@ class Environment(object):
         legal_actions=(self._game.num_distinct_actions(),),
         current_player=(),
         serialized_state=(),
-        distribution=tuple([dist_size])
     )
 
   def action_spec(self):
@@ -497,13 +477,4 @@ class Environment(object):
     """Updates the distribution over the states of the mean field game."""
     assert (
         self._game.get_type().dynamics == pyspiel.GameType.Dynamics.MEAN_FIELD)
-    self._mfg_distribution = mfg_distribution
-
-  def get_partial_distribution(self, info_state, distribution):
-    size = int(np.sqrt(self.game.distribution_tensor_size()))
-
-    x = info_state[:size].index(1)
-    y = info_state[size:2*size].index(1)
-
-    return [distribution[i*size + j] if i >= 0 and i < size and j >= 0 and j < size else 0 
-            for i in range(x-1, x+2) for j in range(y-1, y+2)]
+    self._mfg_distribution = mfg_distribution
\ No newline at end of file
-- 
2.25.1

